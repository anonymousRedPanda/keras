{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a8c7fd-23c5-4c1d-afb4-2c04eb593063",
   "metadata": {},
   "outputs": [],
   "source": [
    "Implement the Continuous Bag of Words (CBOW) Model for the given (textual\n",
    "document 1) using the below steps:\n",
    "a. Data preparation\n",
    "b. Generate training data\n",
    "c. Train model\n",
    "d. Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1103fe2-58ce-428a-82d3-c8c34c8bfa9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.9.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting click (from nltk)\n",
      "  Downloading click-8.3.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: joblib in c:\\users\\vaishnavi\\appdata\\roaming\\python\\python313\\site-packages (from nltk) (1.5.2)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Downloading regex-2025.11.3-cp313-cp313-win_amd64.whl.metadata (41 kB)\n",
      "Collecting tqdm (from nltk)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\vaishnavi\\appdata\\roaming\\python\\python313\\site-packages (from click->nltk) (0.4.6)\n",
      "Downloading nltk-3.9.2-py3-none-any.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 0.3/1.5 MB ? eta -:--:--\n",
      "   ------------- -------------------------- 0.5/1.5 MB 1.1 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 0.8/1.5 MB 1.2 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 1.0/1.5 MB 1.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 1.3 MB/s  0:00:01\n",
      "Downloading regex-2025.11.3-cp313-cp313-win_amd64.whl (277 kB)\n",
      "Downloading click-8.3.0-py3-none-any.whl (107 kB)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm, regex, click, nltk\n",
      "\n",
      "   ---------------------------------------- 0/4 [tqdm]\n",
      "   ---------- ----------------------------- 1/4 [regex]\n",
      "   -------------------- ------------------- 2/4 [click]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ---------------------------------------- 4/4 [nltk]\n",
      "\n",
      "Successfully installed click-8.3.0 nltk-3.9.2 regex-2025.11.3 tqdm-4.67.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))': /packages/8b/00/6e29bb314e271a743170e53649db0fdb8e8ff0b64b4f425f5602f4eb9014/regex-2025.11.3-cp313-cp313-win_amd64.whl.metadata\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df70ca98-523f-49c6-989b-00dcec27b92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step a: Data Preparation\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d07e7500-9271-4a35-bba3-048b03b2010f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Vaishnavi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Vaishnavi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "191bb397-3058-4c33-8d9c-9c5242f363f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['i', 'love', 'natural', 'language', 'processing', 'and', 'i', 'love', 'deep', 'learning']\n"
     ]
    }
   ],
   "source": [
    "# Sample text (you can change it for your own document)\n",
    "text = \"I love natural language processing and I love deep learning\"\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = word_tokenize(text.lower())\n",
    "\n",
    "print(\"Tokens:\", tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6814816a-2ad7-4b0c-a7b8-b03672fbb42c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Word to Index Mapping: {'i': 1, 'love': 2, 'natural': 3, 'language': 4, 'processing': 5, 'and': 6, 'deep': 7, 'learning': 8}\n"
     ]
    }
   ],
   "source": [
    "# Create a word-to-index dictionary\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(tokens)\n",
    "word2idx = tokenizer.word_index\n",
    "idx2word = {v: k for k, v in word2idx.items()}\n",
    "\n",
    "vocab_size = len(word2idx) + 1  # +1 for padding if needed\n",
    "\n",
    "print(\"\\nWord to Index Mapping:\", word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "58c75d74-5652-43b1-9911-3478b173d498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample training data (context → target):\n",
      "['i', 'love', 'language', 'processing'] → natural\n",
      "['love', 'natural', 'processing', 'and'] → language\n",
      "['natural', 'language', 'and', 'i'] → processing\n"
     ]
    }
   ],
   "source": [
    "# Step b: Generate Training Data for CBOW\n",
    "window_size = 2\n",
    "data = []\n",
    "\n",
    "for i in range(window_size, len(tokens) - window_size):\n",
    "    context = []\n",
    "    for j in range(i - window_size, i + window_size + 1):\n",
    "        if j != i:\n",
    "            context.append(word2idx[tokens[j]])\n",
    "    target = word2idx[tokens[i]]\n",
    "    data.append((context, target))\n",
    "\n",
    "print(\"\\nSample training data (context → target):\")\n",
    "for c, t in data[:3]:\n",
    "    print([idx2word[i] for i in c], \"→\", idx2word[t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "89bb643b-d787-491e-b1b9-5eade16fe9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step c: Train CBOW Model using Keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Lambda, Dense\n",
    "import tensorflow.keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "640c6f4b-70e5-47f9-ac73-9d3fc5930a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare input and output\n",
    "contexts = np.array([x[0] for x in data])\n",
    "targets = np.array([x[1] for x in data])\n",
    "targets = to_categorical(targets, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "98e6b9c8-27c4-4613-9783-48ef8388a8b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vaishnavi\\AppData\\Roaming\\Python\\Python313\\site-packages\\keras\\src\\layers\\core\\embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Vaishnavi\\AppData\\Roaming\\Python\\Python313\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:232: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x2b36305a900>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the CBOW model\n",
    "context_input = Input(shape=(2 * window_size,))\n",
    "embedding = Embedding(input_dim=vocab_size, output_dim=8, input_length=2 * window_size)(context_input)\n",
    "avg = Lambda(lambda x: K.mean(x, axis=1))(embedding)\n",
    "output = Dense(vocab_size, activation='softmax')(avg)\n",
    "\n",
    "model = Model(inputs=context_input, outputs=output)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(contexts, targets, epochs=100, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bd036a38-666d-4dac-84dc-38e059412cfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Word Embeddings (each row corresponds to a word):\n",
      "i : [ 0.06297023  0.06411362 -0.04012642  0.01483477 -0.08171912 -0.11863741\n",
      " -0.19943026  0.21736698]\n",
      "love : [ 0.22274497  0.03360213 -0.09703432 -0.02216818  0.0513423   0.22848998\n",
      " -0.0599842   0.02821499]\n",
      "natural : [ 0.10064117 -0.07367028  0.05822498  0.02383697 -0.15704936  0.00652158\n",
      "  0.08021242 -0.05821476]\n",
      "language : [ 0.18608834  0.02572224 -0.19292735 -0.02684356  0.06667159 -0.04226843\n",
      " -0.1634473   0.18225907]\n",
      "processing : [ 0.24213779  0.01662255 -0.05952685  0.00363164  0.00861415  0.18526335\n",
      " -0.14050922  0.06213596]\n",
      "and : [-0.01709026 -0.0552813   0.07804796  0.09277067 -0.07956363 -0.09527668\n",
      " -0.02475209 -0.063516  ]\n",
      "deep : [-0.07836723 -0.05767421 -0.00531642  0.09657529  0.0087047  -0.06305738\n",
      " -0.17726074  0.10483325]\n",
      "learning : [-0.05968617  0.09768292  0.08049244  0.09734058 -0.07399023 -0.12950647\n",
      " -0.10908422  0.13061422]\n"
     ]
    }
   ],
   "source": [
    "# Step d: Output - Display Word Embeddings\n",
    "weights = model.get_weights()[0]\n",
    "print(\"\\nWord Embeddings (each row corresponds to a word):\")\n",
    "for word, idx in word2idx.items():\n",
    "    print(word, \":\", weights[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1311e56-1ea7-45c7-8c9d-9ff75c48b163",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
